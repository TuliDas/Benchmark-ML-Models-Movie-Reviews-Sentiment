{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMDB Movie Reviews â€“ Sentiment Analysis**\n",
        "\n",
        "This project aims to build and evaluate machine learning models for **sentiment classification** of IMDB movie reviews (**positive vs. negative**).  \n",
        "The workflow follows a structured pipeline covering preprocessing, feature engineering, model training, evaluation, and error analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Project Objectives\n",
        "- Preprocess raw text data into clean and usable form.  \n",
        "- Explore two feature extraction methods: **Bag of Words (BoW)** and **TF-IDF**.  \n",
        "- Train and compare multiple models under both **baseline** and **tuned** configurations.  \n",
        "- Select the **best baseline** and **best tuned** model based on **F1-Score**.  \n",
        "- Perform **error analysis** (False Positives & False Negatives) to interpret model behavior.  \n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Workflow Overview\n",
        "1. **Data Preprocessing** â€“ text cleaning, tokenization, vectorization.  \n",
        "2. **Model Training & Evaluation** â€“ baseline vs. tuned models.  \n",
        "3. **Model Selection** â€“ identify best models by F1-Score.  \n",
        "4. **Error Analysis** â€“ compare misclassifications across models.  \n",
        "5. **Visualization & Reporting** â€“ metrics tables, confusion matrices, error breakdown.  \n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Evaluation Metrics\n",
        "- **Accuracy** â€“ overall correctness of predictions.  \n",
        "- **F1-Score** â€“ balance between precision & recall, used for final model selection.  \n",
        "\n",
        "\n",
        "---\n",
        "### Project Functions\n",
        "All project-specific functions are implemented in the `src/` folder.  \n",
        "Each module contains the relevant functions for preprocessing, feature extraction, model training, evaluation, and error analysis."
      ],
      "metadata": {
        "id": "_xqOxtrlEpsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General purpose\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "# Data Preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Model training\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Model selection & evaluation\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "hJ0YhbZxEg2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUYm6OOT5Nzp"
      },
      "outputs": [],
      "source": [
        "!git https://github.com/TuliDas/Benchmark-ML-Models-Movie-Reviews-Sentiment\n",
        "%cd Benchmark-ML-Models-Movie-Reviews-Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"src\")  # Add src folder to Python path to import project modules"
      ],
      "metadata": {
        "id": "0dE-LasSGFy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2gyS6zT4g8d"
      },
      "source": [
        "## Dataset Loading\n",
        "\n",
        "- **Source**: [IMDB Movie Reviews Dataset (50K, Kaggle)](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)  \n",
        "- **Steps**:\n",
        "  1. Download the dataset from Kaggle and upload it to Google Colab.  \n",
        "  2. Load the CSV file into a Pandas DataFrame.  \n",
        "  3. Preserve a deep copy of the original dataset (`data_OG`).  \n",
        "  4. Display the dataset shape and preview the first few rows.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-b52p3hhBUw"
      },
      "outputs": [],
      "source": [
        "csv_path = \"/content/IMDB Dataset.csv\"\n",
        "\n",
        "data = pd.read_csv(csv_path, engine='python', delimiter=',')\n",
        "data_OG = copy.deepcopy(data)\n",
        "\n",
        "print(\"Loaded data shape:\", data.shape)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2IVxdeY9DDO"
      },
      "source": [
        "# **Data Preprocessing**\n",
        "\n",
        "Before proceeding with model training, we will apply the following **eight key preprocessing steps** to clean and prepare the dataset:\n",
        "\n",
        "1. **HTML & bracketed text removal (clean raw text)**\n",
        "\n",
        "2. **Lowercasing + remove numbers and special chars (normalize text)**\n",
        "\n",
        "3. **Handle negations (combine negation + next word to preserve meaning)**\n",
        "\n",
        "4. **Remove extra whitespace (clean spacing for consistent tokens)**\n",
        "\n",
        "5. **Lemmatization (reduce words to their base forms)**\n",
        "\n",
        "6. **Remove stopwords (remove common words that add little meaning)**\n",
        "\n",
        "7. **Remove very short tokens (remove tiny tokens like 'a', 'I' that may remain)**\n",
        "\n",
        "8. **Stemming (optional last step to reduce words further)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87aRaea7jCXH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# If spaCy is not installed\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAXDvdX-8eZB"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.data_preprocessing import preprocess_pipeline\n",
        "\n",
        "# Apply text preprocessing to the 'review' column\n",
        "# See src/data_preprocessing.py for detailed implementation of preprocess_pipeline\n",
        "data['review'] = data['review'].apply(lambda x: preprocess_pipeline(x,use_stemming=True))"
      ],
      "metadata": {
        "id": "H_JI1RoQvv9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObQNhwxrI6K3"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ¯ **Label Encoding**\n",
        "\n",
        "We convert the sentiment labels from text (`\"positive\"`, `\"negative\"`) to numerical format (0 and 1) using `LabelBinarizer`.\n",
        "\n",
        "- **`LabelBinarizer`** transforms categorical labels into binary values:\n",
        "  - 0 for **negative**\n",
        "  - 1 for **positive**\n",
        "\n",
        "This numerical format is required for machine learning models to process the target variable.\n"
      ],
      "metadata": {
        "id": "eTbg_YW63Z_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode sentiment labels to 0 (negative) and 1 (positive)\n",
        "label = LabelBinarizer()\n",
        "sentiment_binary = label.fit_transform(data['sentiment'])"
      ],
      "metadata": {
        "id": "R9pgNXa63kA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§ª Train-Test Split**\n",
        "\n",
        "We split the dataset into training and testing sets to evaluate model performance on unseen data.\n",
        "\n",
        "- **`test_size=0.4`**: 40% of data is reserved for testing, 60% for training.  \n",
        "- **`random_state=42`**: Ensures reproducibility by fixing the random seed.  \n",
        "- **`stratify=data['sentiment']`**: Maintains the same class distribution in both train and test sets to avoid imbalance."
      ],
      "metadata": {
        "id": "x7La3iaV_sto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data['review'],\n",
        "    sentiment_binary,\n",
        "    test_size=0.4,          # 60% train, 40% test split\n",
        "    random_state=42,        # for reproducibility\n",
        "    stratify=data['sentiment']  # keep class balance in splits\n",
        ")"
      ],
      "metadata": {
        "id": "swK5GJ7I_tko",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "PnXNnIC_8owB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Extraction**\n",
        "- Utility function to convert raw text into numerical features using a given vectorizer\n",
        "(e.g., CountVectorizer for BoW, TfidfVectorizer for TF-IDF).\n"
      ],
      "metadata": {
        "id": "0Rif0hGADHGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. CountVectorizer (Bag of Words)**\n",
        "Converts text into token counts with filtering to reduce noise:\n",
        "- min_df=5 â†’ remove rare words  \n",
        "- max_df=0.9 â†’ remove overly common words  \n",
        "- ngram_range=(1,3) â†’ include unigrams, bigrams, trigrams\n",
        "\n",
        "#### **2. TF-IDF Vectorizer**\n",
        "Represents text by term importance:\n",
        "- High weight = frequent in doc but rare across corpus\n",
        "- Captures importance better than raw counts\n",
        "\n"
      ],
      "metadata": {
        "id": "pj9yub4uDK4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `featured_data`\n",
        "Stores train/test splits for different feature extraction methods.  \n",
        "```python\n",
        "featured_data = {\n",
        "    \"BoW\": {\n",
        "        \"train\": <sparse_matrix>,   # X_train after CountVectorizer\n",
        "        \"test\": <sparse_matrix>     # X_test after CountVectorizer\n",
        "    },\n",
        "    \"TF-IDF\": {\n",
        "        \"train\": <sparse_matrix>,   # X_train after TfidfVectorizer\n",
        "        \"test\": <sparse_matrix>     # X_test after TfidfVectorizer\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "LHKlvXm6HIki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.feature_extraction import initialize_featured_data, extract_all_features\n",
        "# See src/feature_extraction.py for function details (`initialize_featured_data` and `extract_all_features`)\n",
        "\n",
        "featured_data = initialize_featured_data()\n",
        "\n",
        "# Extract all features for training and testing data\n",
        "featured_data = extract_all_features(featured_data, X_train, X_test)\n",
        "\n",
        "# Check the keys to see available feature types (e.g., 'BoW', 'TF-IDF')\n",
        "featured_data.keys()"
      ],
      "metadata": {
        "id": "GEAmHiTdXPr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Some sample output\n",
        "print(f\"BoW-train-Shape : {featured_data['BoW']['train'].shape}\")\n",
        "print(f\"TF-IDF-test-Shape : {featured_data['TF-IDF']['test'].shape}\")"
      ],
      "metadata": {
        "id": "RIG6L5gm8Fe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”¹ **Model Training Plan**  \n",
        "\n",
        "We will now train and evaluate multiple machine learning models for sentiment classification.  \n",
        "The goal is to compare different algorithms and feature extraction techniques to determine which combination works best for IMDB movie reviews.  \n",
        "\n",
        "### 2. Models to Train\n",
        "We will train **four classifiers**, each with both BoW and TFâ€“IDF features:  \n",
        "- **Logistic Regression (LR)**  \n",
        "- **Multinomial Naive Bayes (MNB)**  \n",
        "- **Stochastic Gradient Descent Classifier (SGDClassifier)**  \n",
        "- **Support Vector Classifier (SVC)**  "
      ],
      "metadata": {
        "id": "QVQoa-YfKtEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary Structures Used\n",
        "\n",
        "To organize predictions, evaluation scores, and error analysis, we maintain a few structured dictionaries:\n",
        "\n",
        "### 1. `results`\n",
        "Stores predictions and metrics for each model, version, and feature type.  \n",
        "```python\n",
        "results = {\n",
        "    \"LogisticRegression\": {\n",
        "        \"baseline\": {\n",
        "            \"BoW\": {\"predictions\": ..., \"f1-score\": ...},\n",
        "            \"TF-IDF\": {...}\n",
        "        },\n",
        "        \"tuned\": {...}\n",
        "    },\n",
        "    \"LinearSVC\": {...}\n",
        "}\n"
      ],
      "metadata": {
        "id": "kVdHHOSFGe09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.model_training import initialize_model_dict_structure, train_models_and_store_predictions\n",
        "\n",
        "# Initialize structured dictionary to store models and predictions\n",
        "# See src/model_training.py for function details\n",
        "results = initialize_model_dict_structure()\n"
      ],
      "metadata": {
        "id": "oKVPXXfp9z8e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train all baseline models on extracted features and store their predictions in the results dictionary\n",
        "results = train_models_and_store_predictions(results, featured_data, y_train, mode=\"baseline\")"
      ],
      "metadata": {
        "id": "ViUFfBn0Gjr8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Some sample output\n",
        "print(f\"Predictions's Array shape of LogisticRegression-Baseline-BoW model : {results['LogisticRegression']['baseline']['BoW']['predictions'].shape}\")\n",
        "print(f\"Predictions's Array shape of MultinomialNB-Baseline-TF-IDF model : {results['MultinomialNB']['baseline']['TF-IDF']['predictions'].shape}\")"
      ],
      "metadata": {
        "id": "FOkJdQbx8z9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Š Model Evaluation**\n",
        "\n",
        "After training all models with both **Bag of Words (BoW)** and **TF-IDF** features, the next step is to **evaluate their performance**.\n",
        "\n",
        "We will calculate:  \n",
        "\n",
        "1. **Accuracy Score** â€“ indicates the proportion of correctly classified reviews out of all predictions.\n",
        "2. **Classification Report** â€“ Includes **Precision, Recall, F1-score** for each class (positive & negative).  \n",
        "\n",
        "The classification report provides a detailed evaluation of model performance by showing:\n",
        "\n",
        "- **Precision:** How many of the predicted positive reviews are actually positive.  \n",
        "- **Recall:** How many of the actual positive reviews were correctly identified.  \n",
        "- **F1-Score:** Harmonic mean of precision and recall, balancing both metrics.  \n",
        "- **Support:** Number of true instances for each class in the test set."
      ],
      "metadata": {
        "id": "-SBBWd8XpB5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ðŸ”„ Cross-Validation**\n",
        "\n",
        "Cross-validation splits the training data into multiple folds, training and testing the model on different subsets.\n",
        "\n",
        "- Provides a **more reliable estimate** of model performance than a single train-test split."
      ],
      "metadata": {
        "id": "wepyACTtkzKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and store cross-validation accuracy for baseline models\n",
        "from src.model_evaluation import calculate_cross_validation\n",
        "\n",
        "results = calculate_cross_validation(results, featured_data, y_train, mode=\"baseline\")"
      ],
      "metadata": {
        "id": "vz0iBKgy8N_q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Some sample output\n",
        "\n",
        "print(f\"Cross-Validation Score of SGDClassifier-Baseline-BoW model : {results['SGDClassifier']['baseline']['BoW']['cv_accuracy']}\")\n",
        "print(f\"Cross-Validation Score of LinearSVC-Baseline-TF-IDF model : {results['LinearSVC']['baseline']['TF-IDF']['cv_accuracy']}\")"
      ],
      "metadata": {
        "id": "t2iramlO-ATU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate all (Baseline) Models : store Accuracy, classification Reports (Separately Precision, recall, f1-score) and Confusion-Matrix\n",
        "from src.model_evaluation import evaluate_and_update_metrics\n",
        "\n",
        "results = evaluate_and_update_metrics(results, featured_data, y_test, mode=\"baseline\")"
      ],
      "metadata": {
        "id": "fJ0HxJ3i4UIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print and Testing some sample output\n",
        "\n",
        "prec = results['LogisticRegression']['baseline']['BoW']['precision']\n",
        "rec = results['LogisticRegression']['baseline']['BoW']['recall']\n",
        "f1 = results['LogisticRegression']['baseline']['BoW']['f1_score']\n",
        "print(f\"Precision, Recall and F1-score of Logistic Regression model : {prec},{rec},{f1}\")"
      ],
      "metadata": {
        "id": "LKiHPTbd-vm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ“Š Visualize Confusion Matrix\n",
        "\n",
        "A confusion matrix shows how well a classification model performs by displaying counts of:\n",
        "\n",
        "- **True Positives (TP)**: Correctly predicted positive cases  \n",
        "- **True Negatives (TN)**: Correctly predicted negative cases  \n",
        "- **False Positives (FP)**: Incorrectly predicted positives (Type I error)  \n",
        "- **False Negatives (FN)**: Incorrectly predicted negatives (Type II error)\n",
        "\n",
        "It helps understand the types of errors your model makes beyond overall accuracy."
      ],
      "metadata": {
        "id": "kuL-tkcPvoMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize confusion matrices for all baseline models and feature types\n",
        "from src.utils import visualize_confusion_matrix\n",
        "visualize_confusion_matrix(results, featured_data, mode=\"baseline\")"
      ],
      "metadata": {
        "id": "m4LWSuuhC-Sx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”§ Hyperparameter Tuning of Models\n",
        "\n",
        "After establishing baseline performance, the next step is **hyperparameter tuning** to optimize each model.  \n",
        "\n",
        "We will use **GridSearchCV** on both **Bag of Words (BoW)** and **TF-IDF** features to systematically search for the best parameters.  \n",
        "\n",
        "### Steps:\n",
        "1. **Define parameter grids** for each model (Logistic Regression, Multinomial Naive Bayes, SGDClassifier, and LinearSVC).  \n",
        "2. **Run GridSearchCV** to perform cross-validation and find the best parameters.  \n",
        "3. **Retrain each model** using the best parameters found.  \n"
      ],
      "metadata": {
        "id": "ghOKgBdY11QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tune all models store tuned predictions\n",
        "from src.hyperparameter_tuning import get_hyperparameter_grids, tune_all_models\n",
        "param_grids = get_hyperparameter_grids()\n",
        "results = tune_all_models(results, featured_data, param_grids, y_train)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8hKQX7Aq5N2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Some sample output\n",
        "print(f\"Predictions's Array shape of SGDClassifier-Tuned-BoW model : {results['SGDClassifier']['tuned']['BoW']['predictions'].shape}\")\n",
        "print(f\"Predictions's Array shape of LinearSVC-Tuned-TF-IDF model : {results['LinearSVC']['tuned']['TF-IDF']['predictions'].shape}\")"
      ],
      "metadata": {
        "id": "hsaqnNZ0_uNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate all (Tuned) Models : store Accuracy, classification Reports (Separately Precision, recall, f1-score) and Confusion-Matrix\n",
        "results = evaluate_and_update_metrics(results,featured_data, y_test, mode=\"tuned\")"
      ],
      "metadata": {
        "id": "AYrzxU-jZhu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize confusion matrices for all tuned models\n",
        "visualize_confusion_matrix(results,featured_data, mode=\"tuned\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dgxaYl5tEfNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Performance Comparison**\n",
        "In this section, we consolidate the evaluation results of all models and feature types into a single **comparison DataFrame**.\n",
        "This provides a structured view of **baseline vs. tuned** performance for each model across **BoW and TF-IDF** representations.\n"
      ],
      "metadata": {
        "id": "x2kogSnYy2j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing results across models with featured data\n",
        "from src.model_performance_comparison import create_comparison_dataframe\n",
        "\n",
        "df_results = create_comparison_dataframe(results, featured_data)\n",
        "df_results\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2Iqb6J5v0rtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize model comparison table\n",
        "from src.utils import visualize_model_comparison\n",
        "\n",
        "visualize_model_comparison(df_results)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MRbHFenz8qxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”Ž Identifying Models to Compare\n",
        "\n",
        "During model selection, we observed that both the **best baseline** and the **best tuned** models turned out to be the same:  \n",
        "**LinearSVC with TF-IDF features**, achieving the highest F1-Score overall.  \n",
        "\n",
        "Since comparing two identical models does not provide meaningful insights, we adopt the following strategy:  \n",
        "\n",
        "1. **Best Tuned Model**  \n",
        "   - Selected strictly by highest F1-Score.  \n",
        "   - This is always a **TF-IDFâ€“based tuned model** (LinearSVC).  \n",
        "\n",
        "2. **Baseline Model for Comparison**  \n",
        "   - To ensure diversity, we restrict the baseline to use a **different feature representation (BoW)**.  \n",
        "   - We then select the **best-performing baseline model** within BoW features (Logistic Regression).  \n",
        "\n",
        "This approach allows us to compare:  \n",
        "- **LinearSVC + TF-IDF (tuned)** vs. **Logistic Regression + BoW (baseline)**  \n",
        "\n",
        "âœ… This ensures a fairer and more insightful comparison between a strong tuned model and a simpler baseline with different features.\n"
      ],
      "metadata": {
        "id": "iAxWaHYiAEBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `best_two_models` (Sample Structure)\n",
        "\n",
        "```python\n",
        "best_two_models = {\n",
        "    \"baseline\": {\"full-name\": \"...\", \"predictions\": [...], \"f1-score\": ...},\n",
        "    \"tuned\":    {\"full-name\": \"...\", \"predictions\": [...], \"f1-score\": ...}\n",
        "}\n"
      ],
      "metadata": {
        "id": "JGGl19XKIohA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.select_best_models import get_best_baseline_and_tuned\n",
        "best_two_models = get_best_baseline_and_tuned(df_results, results, metric=\"F1-Score\")\n",
        "\n",
        "for _,info in best_two_models.items():\n",
        "  print(f\"{info['full-name']} : F1-Score = {info['f1-score']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7RXlcN-1LKBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Error Analysis Initialization & Computation**\n",
        "\n",
        "1. **Initialize Error Analysis Dictionary**  \n",
        "   Using `initialize_error_analysis_dict(best_two_models)`, we create a structured dictionary to store:\n",
        "   - Model details (`name`, `feature`, `f1_score`, etc.)\n",
        "   - False positives (`false_positives`)\n",
        "   - False negatives (`false_negatives`)\n",
        "   - Their corresponding indexes (`fp_indexes`, `fn_indexes`)\n",
        "\n",
        "2. **Compute FP & FN for Best Models**  \n",
        "   `compute_bestModels_all_fp_fn()` fills the dictionary with the false positives and false negatives for:\n",
        "   - Best baseline model\n",
        "   - Best tuned model  \n",
        "\n",
        "3. **Quick Overview**  \n",
        "   We print the counts of FP and FN for each model/version to get an immediate sense of misclassifications.\n"
      ],
      "metadata": {
        "id": "hdXFnQ8AxB0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `error_analysis_dict` (Sample Structure)\n",
        "\n",
        "```python\n",
        "error_analysis_dict = {\n",
        "    \"baseline\": { \"full-name\": \"...\", \"name\": \"...\", \"false_positives\": \"..\",\n",
        "        \"false_negatives\": \"..\", \"fp_indexes\": \"..\", \"fn_indexes\": \"..\"\n",
        "    },\n",
        "    \"tuned\": {\n",
        "        \"full-name\": \"...\", ............ \"fn_indexes\": \"..\"\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "1FjrVT5bJG7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.error_analysis import initialize_error_analysis_dict, compute_bestModels_all_fp_fn\n",
        "\n",
        "error_analysis_dict = initialize_error_analysis_dict(best_two_models)\n",
        "error_analysis_dict = compute_bestModels_all_fp_fn(error_analysis_dict, best_two_models, data_OG, data, X_test, y_test)\n",
        "\n",
        "for version, info in error_analysis_dict.items():\n",
        "  print(f\"{info['name']}-{version}-({info['feature']}) : FP = {len(info['false_positives'])} , FN = {len(info['false_negatives'])}\")"
      ],
      "metadata": {
        "id": "-qV-YU_jttLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing False Positives and False Negatives to Text Files\n",
        "\n",
        "For deeper insight into model misclassifications, we focus on the **two selected models**:\n",
        "\n",
        "1. **LinearSVC â€“ Tuned â€“ (TF-IDF)** (best overall model by F1-score)  \n",
        "2. **Logistic Regression â€“ Baseline â€“ (BoW)** (best baseline with different feature representation)  \n",
        "\n",
        "We generate **three text files** for analysis:\n",
        "\n",
        "1. **FP & FN detected by both models** â€“ Reviews misclassified by **both models**.  \n",
        "2. **FP & FN detected by Logistic Regression only** â€“ Errors unique to the **baseline BoW model**.  \n",
        "3. **FP & FN detected by LinearSVC only** â€“ Errors unique to the **tuned TF-IDF model**.  \n",
        "\n",
        "This approach helps us identify:  \n",
        "- **Common hard cases** that challenge both models.  \n",
        "- **Model-specific weaknesses**, highlighting where one model struggles while the other succeeds.  \n",
        "- **Error patterns** that may guide future improvements in preprocessing, feature extraction, or model design.  \n"
      ],
      "metadata": {
        "id": "8Vc6PHGexFCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `separate_fp_fn_df` (Sample Structure)\n",
        "\n",
        "```python\n",
        "separate_fp_fn_df = {\n",
        "    \"both_models\": {  \"file-name\": textfile name,\n",
        "                 \"fp\": dataframe of fp,\n",
        "                 \"fn\": dataframe of fn\": \"..\"\n",
        "    },\n",
        "    \"only_baseline\": {\n",
        "            \"file-name\": textfile name,  ......\n",
        "    },\n",
        "    \"only_tuned\" : { ......\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "68bePaHCKdR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.error_analysis import generate_error_analysis_separate_dataFrames\n",
        "\n",
        "baseline_name = error_analysis_dict['baseline']['full-name']\n",
        "tuned_name = error_analysis_dict['tuned']['full-name']\n",
        "\n",
        "separate_fp_fn_df = generate_error_analysis_separate_dataFrames(error_analysis_dict,baseline_name, tuned_name)\n",
        "\n",
        "for key, value in separate_fp_fn_df.items():\n",
        "  print(f\"{key} : FP = {len(value['fp'])} , FN = {len(value['fn'])}\")"
      ],
      "metadata": {
        "id": "xrqyIuvvyO-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.reporting import create_three_text_files\n",
        "create_three_text_files(separate_fp_fn_df)"
      ],
      "metadata": {
        "id": "ww4hxBHvwAFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}